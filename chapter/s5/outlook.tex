\section{Discussion and Outlook}
During this academic research, we have found more questions than answers in various fields. Currently, we have successfully applied model-free reinforcement learning and imitation learning in mobile robot navigation for both indoor and outdoor environments, considered the prediction and memory ability of neural networks and tackled the domain adaptation problem of the trained policy. However, there are still several less-addressed issues:
(1) The training of model-free reinforcement requires a large number of samples. Thousands of pieces of computation hardware and highly parallelised and distributed threads become necessary, especially for deep reinforcement learning \cite{barthmaron2018distributional, espeholt2018impala}. (2) Without implicitly or explicitly predicting the future states of the dynamic environment \cite{alahi2016social, chen2018crowd}, it is still challenging to generate an efficient navigation strategy among crowded pedestrians. And (3) Effective metrics to measure the visual-based autonomous driving in the real world is still missing.

Here, we briefly summarise the related future works for both deep reinforcement learning and imitation learning for navigation policies considering the two aforementioned issues.

For deep reinforcement learning, model-based algorithms are generally considered to be sample efficient \cite{deisenroth2013survey}. Predicting the forward image flow directly based on historical image states and actions was considered in \cite{finn2017deep}. They collected data through random motions of manipulators to estimate the visual-based dynamics model. This forward model was combined with model predictive control to finish several manipulation tasks. It can also be generalized to unknown objects.
Further, background occlusions \cite{ebert2017self} and registrations with start and target images \cite{ebert2018robustness} were considered to solve longer-moving and more complicated tasks.
Beyond leveraging the model with MPC, other research \cite{nagabandi2018neural} has let the model-free methods first imitate the behaviour of a model-based method. Combining model-free and model-based methods, TRPO \cite{schulman2015trust} was also improved with a learned dynamic model in \cite{kurutach2018model}. It yielded a much more stable learning process compared with the vanilla model-based reinforcement learning methods.
However, model-based reinforcement learning always lags behind the best model-free algorithms, especially when learning the model through high-dimensional approximators like deep neural networks. Employing uncertainty-aware dynamics models with sampling-based uncertainty propagation \cite{chua2018deep} has achieved comparable results to the best model-free algorithms on several challenging benchmark tasks. Extensions of model-based deep reinforcement learning on ground robot navigation would be exciting.

To predict the future state of nearby dynamic agents, learning a sensory-dynamics-model for autonomous driving is necessary. Considering the high dimension of RGB images, building this model on a top-view 3D lidar and first-person view semantic segmentation information through imitation learning may be a potential direction \cite{Rhinehart_2018_ECCV, rhinehart2018deep}.
% Semantics segmentation information should be enough to generate effective driving policy, and it can be naturally transferred to the real world \cite{pan2017virtual, yang2018eccv}.
% based on pre-trained semantic segmentation models \cite{he2017mask}.
Also, how to train a forward dynamics model efficiently with limited data is rarely considered. Xu \textit{et al.} \cite{xu2018algorithmic} introduced a novel algorithmic framework for model-based reinforcement learning algorithms with theoretical guarantees. This is particularly critical for mobile robot tasks. In terms of behaviour cloning, leveraging the implicit information but not just the raw sensor input may improve the efficiency of the model training. For example, gaze information has been proved to speed up the training of novice human learners.
Imposing this kind of information \cite{chen2019gaze, liu2019gaze} may also encourage the evolution of imitation learning for navigation.

In Part \ref{part3}, we have to measure the \textit{real-to-sim} pipeline for outdoor autonomous driving in another extreme simulated environment finally, because it is not trivial to achieve quantative results for this outdoor driving condition in the real world. An effective metric for the visual-based outdoor autonomous driving, especially for learning-based method, is still an open problem.
