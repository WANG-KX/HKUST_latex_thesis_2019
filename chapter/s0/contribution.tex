\section{Contributions}
% % \subsection{Markov Decision Process}
% We formalize a robotics task (e.g., navigation, manipulation) as a \textit{Markov Decision Process} (MDP), in which the agent interacts with the environment through a sequence of observations, actions, and reward signals. An MDP is a $5-$tuple $\left< \mathcal{S}, \mathcal{A}, P, \mathcal{R}, \gamma \right>$:
% $\mathcal{S}$: set of all states.
% $\mathcal{A}$: set of all actions.
%   %\item $\mathcal{P}(\bs_{t+1}|\bs_{t}, \ba_{t})$: defines the probability of transisting to state $\bs_{t+1}$ from a state-action pair $(\bs_{t}, \ba_{t})$.
%   %\item $\mathcal{P}(S_{t+1}|S_{t}, A_{t})$: defines the probability of transisting to a next state from a state-action pair.
% $\mathcal{P}$: the transition dynamics,
% where $P(\bs'|\bs, \ba)$ defines the distribution of the next state $\bs'$ by taking action $\ba$ in state $\bs$,
% where $\bs,\bs'\in\mathcal{S}, \ba\in\mathcal{A}$. We also denote the initial state distribution $P(\bs_0)$ as $\rho_0$.
%   %\item $\mathcal{R}(\bs_{t}, \ba_{t})$: defines the instantaneous scalar reward received by the agent from a state-action pair $(\bs_{t}, \ba_{t})$.
%   %\item $\mathcal{R}(S_{t}, A_{t})$: defines the instantaneous scalar reward received by the agent from a state-action pair.
%   % \item $\mathcal{R}(\bs_t,\ba_t,\bs_{t+1})$ \hl{check again here}: defines the instantaneous scalar reward received by the agent from a state-action pair. Usually one is interested in the expected reward achieved with a state-action pair: $R_{t+1}(\bs_t,\ba_t)=\mathbf{E}_{\bs_{t+1}\sim\mathcal{P}(\cdot|\bs_t,\ba_t)}\left[\mathcal{R}(\bs_t,\ba_t,\bs_{t+1})\right]$; in the following descriptions, we mostly use $R_{t+1}$ to be short for $R_{t+1}(\bs_t,\ba_t)$.
% $\mathcal{R}$: set of all possible rewards.
%   In the following, we denote the instantaneous scalar reward received by the agent by taking action $\ba_t$ from state $\bs_t$ as $R_{t+1}(\bs_t,\ba_t)$,
%   and use $R_{t+1}$ as short for $R_{t+1}(\bs_t,\ba_t)$.
%   There also exist other definitions of the reward function that depend only on the state itself,
%   in which $R(\bs)$ refers to the reward signal that the agent receives by arriving at state $\bs$.
%   In some of the following discussions, the negative counterpart of the reward function,
%   the cost function, is used, and is denoted as $c(\bs)$.
% $\gamma$: a discount factor in the range of $[0,1]$.
%
% In an MDP, the agent takes an action $\ba_{t}$ in state $\bs_{t}$, receives a reward $R_{t+1}$, and transits to the next state $\bs_{t+1}$ following the transition dynamics $\mathcal{P}(\bs_{t+1}|\bs_{t},\ba_{t})$.
%
% In robotics, we mainly consider \textit{episodic} MDPs, where there exists a \textit{terminal state} (e.g., a mobile ground vehicle reaches a certain goal location, a manipulator successfully grabs a red cup) that, once reached, terminates the current \textit{episode}. Also for an \textit{episodic} MDP with a time horizon of $T$, an \textit{episode} will still be terminated after a maximum of $T$ time steps, even if by then the \textit{terminal state} has not yet been reached.
%
% Another point worth mentioning is the \textit{partial observability}.
% In a robotics task, an autonomous agent perceives the world with its onboard sensor (e.g., RGB/depth camera, IMU, laser range sensor, 3D Lidar),
% receiving one observation
% %$\mathbf{x}_{t}$ (e.g., one color/depth image, one laser scan, one ... point cloud)
% per time step. However, simply representing $\bs_{t}$ by $\mathbf{x}_{t}$ often does not satisfy the \textit{Markov} property: one such sensor reading can hardly capture all the necessary information for the agent to make decisions in the future,
% in which case the underlying procedure is called a Partial Observeble MDP (POMDP).
% This is often dealt with by either stacking several (e.g, $N$) consecutive observations $\{ \mathbf{x}_{t-N+1}, \mathbf{x}_{t-N+2}, \dots, \mathbf{x}_{t} \}$ to represent $\bs_{t}$, or by feeding $\mathbf{x}_{t}$ into a \textit{recurrent} neural network instead of a \textit{feed forward} one,
% such that the past information is naturally accounted with (e.g., by the cell state when using the long short-term memories (LSTMs).
%
% Reinforcement learning agents are designed to learn from interactions how to behave to achieve a certain goal \cite{sutton1998reinforcement}.
% More precisely, here the objective of learning is to maximize the \textit{expected discounted return},
% where the \textit{discounted return} is defined as follows:
% \begin{align}
%   G_{t}
% &=
%   R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{T-t-1} R_{T}
% \\&=
%   \sum_{k=t}^{T} \gamma^{k-t} R_{k+1}.
% \end{align}
%
% To learnin sensorimotor, two important definitions are introduced:
% \begin{itemize}
%   \item \textbf{Policies}: $\pi, \mu$
%     \begin{itemize}
%       \item $\pi(\ba|\bs)$: stochastic policy, where actions are drawn from a probability distribution defined by $\pi(\ba|\bs)$.
%       \item $\mu(\bs)$: determinstic policy, where actions are deterministically selected for a given state $\bs$.
%     \end{itemize}
%   \item \textbf{Value functions}: $V, Q$
%     \begin{itemize}
%       \item $V^{\pi}(\bs)$: state-value function, defined as the expected return when starting from state $\bs$ and following policy $\pi$ thereafter: % estimates how good it is to be in a particular state $\bs$ following policy $\pi$
%       \begin{align}
%         V^{\pi}(\bs)
%       &=
%         \mathbb{E}_{\pi}\left[ G_t | \bs_t=\bs \right]
%       \\&=
%         \mathbb{E}_{\pi}\left[ \sum_{k=t}^{T}\gamma^{k-t} R_{k+1} | \bs_t=\bs \right].
%         \label{equ:value-reward}
%       \end{align}
%       \item $Q^{\pi}(\bs, \ba)$: action-value function,
%       defined as the expected return by taking the action $\ba$ from state $\bs$,
%       then following $\pi$ thereafter: % estimates how good it is to take a particular action $\ba$ in a particular state $\bs$ following policy $\pi$
%       \begin{align}
%         Q^{\pi}(\bs,\ba)
%       &=
%         \mathbb{E}_{\pi}\left[ G_t | \bs_t=\bs, \ba_t=\ba \right]
%       \\&=
%         \mathbb{E}_{\pi}\left[ \sum_{k=t}^{T}\gamma^{k-t} R_{k+1} | \bs_t=\bs, \ba_t=\ba \right].
%         \label{equ:q-reward}
%       \end{align}
%       \item $Q^{*}(\bs, \ba)$: optimal value function (We omit the case for state-value function $V$ here since the action-value function $Q$ is a much more effective representation for control.):
%       \begin{align}
% %        V_{*}(\bs)
% %      &=
% %        \max_{\pi} V_{\pi}(\bs)
% %      \\
%         Q^{*}(\bs,\ba)
%       &=
%         \max_{\pi} Q^{\pi}(\bs,\ba).
%       \end{align}
%       \item $\pi^{*}(\ba|\bs)$: optimal policy:
%       \begin{align}
%         \pi^{*}(\ba|\bs)
%       &=
%         {\argmax}_{\ba} Q^{*}(\bs,\ba).
%       \end{align}
%     \end{itemize}
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%  contributions  %%%%%%%%%%%%%%%%%%%%%%%%
The contributions of this thesis are as follows:
\begin{itemize}
  \item In Chapter \ref{ch:intro}, we discuss the evolution of learning-based methods on robotics. Compared with pure supervised learning, robotics tasks are normally defined as deep reinforcement learning or inverse imitation learning. We summarised the same discussions in \cite{tai2016survey} before.
  \item In Chapter \ref{ch:obstacle}, we propose active obstacle avoidance through deep reinforcement learning. Even though the network is initialized by weights trained from supervised learning, the online learning process of reinforcement learning encourages the model to explore more robust feature patterns in the network. This works was previously presented in \cite{tl_rcar_2016} and \cite{tai2016towards}.
  \item In Chapter \ref{ch:mfrl}, we consider model-free reinforcement learning to tackle the mapless navigation of mobile robots. The robot successfully navigates to various targets in an indoor environment based on local lidar sensor information and coarse target guidance. This work was published previously as \cite{tai2017virtual}.
  \item In Chapter \ref{ch:il}, we consider GAN-style imitation learning to learn complex socially compliant navigation behaviours among pedestrians. This work was described previously in \cite{tai2018social}.
  \item In Chapter \ref{ch:vrg}, we propose a new shift loss, which enables generating consistent synthetic image streams without imposing temporal constraints, or even sequential training data. Additionally, since we decouple the policy training and the adaptation operations, the preparations for transferring the polices from simulation to the real world can be conducted in parallel with the training of the control policies. This work was previously published as \cite{zhang2019vrgoggles}.
  \item In Chapter \ref{ch:uail}, we follow the recent real-to-sim pipeline by translating a testing world image back to the training domain when using a trained policy. In the translation process, a stochastic generator is used to generate various images stylized under the training domain randomly or directionally. Through the uncertainty-aware imitation learning policy, we can easily choose the safest policy with the lowest uncertainty among the generated images. This work was presented before as \cite{tai2019end}.
\end{itemize}
